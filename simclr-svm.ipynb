{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 🔁 SimCLR + SVM on UNSW-NB15 with Progress Output\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\ntrain = pd.read_parquet(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.parquet\")\ntest = pd.read_parquet(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.parquet\")\ndf = pd.concat([train, test])\ndf = df.drop(columns=['id', 'attack_cat'], errors='ignore')\nfor col in ['proto', 'service', 'state']:\n    df[col] = pd.factorize(df[col])[0]\nX = df.drop('label', axis=1)\ny = df['label']\nX = StandardScaler().fit_transform(X)\n\ndef build_encoder(input_dim):\n    inputs = layers.Input(shape=(input_dim,))\n    x = layers.Dense(512, activation='relu')(inputs)\n    x = layers.Dense(256, activation='relu')(x)\n    return models.Model(inputs, x)\n\ndef build_projection_head(input_dim):\n    inputs = layers.Input(shape=(input_dim,))\n    x = layers.Dense(128, activation='relu')(inputs)\n    x = layers.Dense(64)(x)\n    return models.Model(inputs, x)\n\ndef nt_xent_loss(z_i, z_j, temperature=0.5):\n    z_i = tf.math.l2_normalize(z_i, axis=1)\n    z_j = tf.math.l2_normalize(z_j, axis=1)\n    representations = tf.concat([z_i, z_j], axis=0)\n    similarity_matrix = tf.matmul(representations, representations, transpose_b=True)\n    logits = similarity_matrix / temperature\n    batch_size = tf.shape(z_i)[0]\n    mask = tf.eye(2 * batch_size)\n    logits = logits * (1 - mask) - 1e9 * mask\n    labels = tf.concat([tf.range(batch_size, 2 * batch_size), tf.range(0, batch_size)], axis=0)\n    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n    return tf.reduce_mean(loss)\n\ndef augment(X):\n    noise = np.random.normal(0, 0.1, X.shape)\n    mask = np.random.binomial(1, 0.85, X.shape)\n    return X * mask + noise\n\ndef train_simclr(X_train, epochs=20, batch_size=512):\n    encoder = build_encoder(X_train.shape[1])\n    projector = build_projection_head(encoder.output_shape[1])\n    model_input = tf.keras.Input(shape=(X_train.shape[1],))\n    features = encoder(model_input)\n    projections = projector(features)\n    simclr_model = tf.keras.Model(model_input, projections)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n\n    for epoch in range(epochs):\n        idx = np.random.permutation(len(X_train))\n        X_train_shuffled = X_train[idx]\n        for i in range(0, len(X_train), batch_size):\n            batch = X_train_shuffled[i:i+batch_size]\n            if batch.shape[0] < 2:\n                continue\n            x1 = augment(batch)\n            x2 = augment(batch)\n            with tf.GradientTape() as tape:\n                z1 = simclr_model(x1, training=True)\n                z2 = simclr_model(x2, training=True)\n                loss = nt_xent_loss(z1, z2)\n            grads = tape.gradient(loss, simclr_model.trainable_variables)\n            optimizer.apply_gradients(zip(grads, simclr_model.trainable_variables))\n        print(f\" Epoch {epoch+1}/20, Loss: {loss.numpy():.4f}\")\n    return encoder\n\ndef evaluate_with_svm(encoder, X_train, X_test, y_train, y_test):\n    X_train_embedded = encoder.predict(X_train)\n    X_test_embedded = encoder.predict(X_test)\n    clf = SVC(kernel='rbf', class_weight='balanced')\n    clf.fit(X_train_embedded, y_train)\n    y_pred = clf.predict(X_test_embedded)\n    report = classification_report(y_test, y_pred, output_dict=True)\n    return {\n        'accuracy': report['accuracy'],\n        'f1_class_0': report['0']['f1-score'],\n        'f1_class_1': report['1']['f1-score']\n    }\n\nresults = []\nratios = [10,20,30,40,50,60,70,80,90]\n\nfor idx, ratio in enumerate(ratios):\n    print(f\"\\n🔁 Combo {idx + 1} → Train:Test = {ratio}:{100 - ratio}\")\n    X_tr, X_te, y_tr, y_te = train_test_split(\n        X, y, test_size=(100 - ratio) / 100,\n        stratify=y, random_state=42\n    )\n    encoder = train_simclr(X_tr.astype(np.float32), epochs=20, batch_size=1024)\n    metrics = evaluate_with_svm(encoder, X_tr, X_te, y_tr, y_te)\n    metrics.update({'train_ratio': ratio})\n    results.append(metrics)\n    print(f\"✅ Finished Combo {idx + 1}: Accuracy = {metrics['accuracy']:.4f}, \"\n          f\"F1 Class 0 = {metrics['f1_class_0']:.4f}, F1 Class 1 = {metrics['f1_class_1']:.4f}\")\n\n# Save results\npd.DataFrame(results).to_csv(\"simclr_svm_unsw_results.csv\", index=False)\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load the results\nresults_df = pd.read_csv(\"simclr_svm_unsw_results.csv\")\n\n# Plot Accuracy and F1 Scores\nplt.figure(figsize=(12, 6))\nplt.plot(results_df['train_ratio'], results_df['accuracy'], marker='o', label='Accuracy')\nplt.plot(results_df['train_ratio'], results_df['f1_class_0'], marker='s', label='F1 Score (Class 0)')\nplt.plot(results_df['train_ratio'], results_df['f1_class_1'], marker='^', label='F1 Score (Class 1)')\n\nplt.title('SimCLR + SVM Performance on UNSW-NB15')\nplt.xlabel('Train Ratio (%)')\nplt.ylabel('Score')\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T20:04:41.324994Z","iopub.execute_input":"2025-07-14T20:04:41.325530Z"}},"outputs":[{"name":"stderr","text":"2025-07-14 20:04:44.508720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752523484.876805      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752523484.984510      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n🔁 Combo 1 → Train:Test = 10:90\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1752523503.810266      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1752523503.810998      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":" Epoch 1/20, Loss: 4.2951\n Epoch 2/20, Loss: 4.2755\n Epoch 3/20, Loss: 4.2187\n Epoch 4/20, Loss: 4.2061\n Epoch 5/20, Loss: 4.1676\n Epoch 6/20, Loss: 4.1807\n Epoch 7/20, Loss: 4.1867\n Epoch 8/20, Loss: 4.1803\n Epoch 9/20, Loss: 4.1950\n Epoch 10/20, Loss: 4.1211\n Epoch 11/20, Loss: 4.1519\n Epoch 12/20, Loss: 4.1489\n Epoch 13/20, Loss: 4.1165\n Epoch 14/20, Loss: 4.1299\n Epoch 15/20, Loss: 4.1226\n Epoch 16/20, Loss: 4.1225\n Epoch 17/20, Loss: 4.1946\n Epoch 18/20, Loss: 4.1302\n Epoch 19/20, Loss: 4.1419\n Epoch 20/20, Loss: 4.0988\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1752523540.991025     106 service.cc:148] XLA service 0x7d342c001d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1752523540.992292     106 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1752523540.992312     106 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1752523541.108045     106 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m120/806\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1752523541.390637     106 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n\u001b[1m7248/7248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1ms/step\n✅ Finished Combo 1: Accuracy = 0.8816, F1 Class 0 = 0.8436, F1 Class 1 = 0.9048\n\n🔁 Combo 2 → Train:Test = 20:80\n Epoch 1/20, Loss: 4.9547\n Epoch 2/20, Loss: 4.9080\n Epoch 3/20, Loss: 4.8634\n Epoch 4/20, Loss: 4.8796\n Epoch 5/20, Loss: 4.8401\n Epoch 6/20, Loss: 4.8391\n Epoch 7/20, Loss: 4.8511\n Epoch 8/20, Loss: 4.8041\n Epoch 9/20, Loss: 4.8164\n Epoch 10/20, Loss: 4.8078\n Epoch 11/20, Loss: 4.8307\n Epoch 12/20, Loss: 4.8139\n Epoch 13/20, Loss: 4.7850\n Epoch 14/20, Loss: 4.8138\n Epoch 15/20, Loss: 4.8081\n Epoch 16/20, Loss: 4.7773\n Epoch 17/20, Loss: 4.7720\n Epoch 18/20, Loss: 4.8064\n Epoch 19/20, Loss: 4.8014\n Epoch 20/20, Loss: 4.8193\n\u001b[1m1611/1611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n\u001b[1m6442/6442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step\n✅ Finished Combo 2: Accuracy = 0.8823, F1 Class 0 = 0.8474, F1 Class 1 = 0.9042\n\n🔁 Combo 3 → Train:Test = 30:70\n Epoch 1/20, Loss: 5.3417\n Epoch 2/20, Loss: 5.2983\n Epoch 3/20, Loss: 5.2542\n Epoch 4/20, Loss: 5.2387\n Epoch 5/20, Loss: 5.2344\n Epoch 6/20, Loss: 5.2275\n Epoch 7/20, Loss: 5.2264\n Epoch 8/20, Loss: 5.1894\n Epoch 9/20, Loss: 5.2171\n Epoch 10/20, Loss: 5.1529\n Epoch 11/20, Loss: 5.2078\n Epoch 12/20, Loss: 5.2083\n Epoch 13/20, Loss: 5.2111\n Epoch 14/20, Loss: 5.1791\n Epoch 15/20, Loss: 5.2073\n Epoch 16/20, Loss: 5.2110\n Epoch 17/20, Loss: 5.1836\n Epoch 18/20, Loss: 5.2054\n Epoch 19/20, Loss: 5.1918\n Epoch 20/20, Loss: 5.1957\n\u001b[1m2416/2416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n\u001b[1m5637/5637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step\n✅ Finished Combo 3: Accuracy = 0.8824, F1 Class 0 = 0.8484, F1 Class 1 = 0.9039\n\n🔁 Combo 4 → Train:Test = 40:60\n Epoch 1/20, Loss: 5.6026\n Epoch 2/20, Loss: 5.5566\n Epoch 3/20, Loss: 5.5109\n Epoch 4/20, Loss: 5.5032\n Epoch 5/20, Loss: 5.4919\n Epoch 6/20, Loss: 5.5187\n Epoch 7/20, Loss: 5.4918\n Epoch 8/20, Loss: 5.4944\n Epoch 9/20, Loss: 5.5005\n Epoch 10/20, Loss: 5.4720\n Epoch 11/20, Loss: 5.4900\n Epoch 12/20, Loss: 5.4525\n Epoch 13/20, Loss: 5.4761\n Epoch 14/20, Loss: 5.4743\n Epoch 15/20, Loss: 5.4837\n Epoch 16/20, Loss: 5.4809\n Epoch 17/20, Loss: 5.4836\n Epoch 18/20, Loss: 5.4681\n Epoch 19/20, Loss: 5.4520\n Epoch 20/20, Loss: 5.4566\n\u001b[1m3221/3221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n\u001b[1m4832/4832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n✅ Finished Combo 4: Accuracy = 0.8878, F1 Class 0 = 0.8569, F1 Class 1 = 0.9077\n\n🔁 Combo 5 → Train:Test = 50:50\n Epoch 1/20, Loss: 5.8075\n Epoch 2/20, Loss: 5.7634\n Epoch 3/20, Loss: 5.7421\n Epoch 4/20, Loss: 5.7199\n Epoch 5/20, Loss: 5.7386\n Epoch 6/20, Loss: 5.7301\n Epoch 7/20, Loss: 5.6898\n Epoch 8/20, Loss: 5.7223\n Epoch 9/20, Loss: 5.6906\n Epoch 10/20, Loss: 5.7004\n Epoch 11/20, Loss: 5.7040\n Epoch 12/20, Loss: 5.7059\n Epoch 13/20, Loss: 5.6906\n Epoch 14/20, Loss: 5.6852\n Epoch 15/20, Loss: 5.6920\n Epoch 16/20, Loss: 5.6849\n Epoch 17/20, Loss: 5.6798\n Epoch 18/20, Loss: 5.6754\n Epoch 19/20, Loss: 5.6771\n Epoch 20/20, Loss: 5.6765\n\u001b[1m4027/4027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step\n\u001b[1m4027/4027\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step\n✅ Finished Combo 5: Accuracy = 0.8854, F1 Class 0 = 0.8541, F1 Class 1 = 0.9057\n\n🔁 Combo 6 → Train:Test = 60:40\n Epoch 1/20, Loss: 5.9769\n Epoch 2/20, Loss: 5.9381\n Epoch 3/20, Loss: 5.9180\n Epoch 4/20, Loss: 5.9139\n Epoch 5/20, Loss: 5.8785\n Epoch 6/20, Loss: 5.8761\n Epoch 7/20, Loss: 5.8857\n Epoch 8/20, Loss: 5.8885\n Epoch 9/20, Loss: 5.8744\n Epoch 10/20, Loss: 5.8736\n Epoch 11/20, Loss: 5.8769\n Epoch 12/20, Loss: 5.8882\n Epoch 13/20, Loss: 5.8836\n Epoch 14/20, Loss: 5.8752\n Epoch 15/20, Loss: 5.8843\n Epoch 16/20, Loss: 5.8799\n Epoch 17/20, Loss: 5.8639\n Epoch 18/20, Loss: 5.8584\n Epoch 19/20, Loss: 5.8511\n Epoch 20/20, Loss: 5.8657\n\u001b[1m4832/4832\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n\u001b[1m3221/3221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n✅ Finished Combo 6: Accuracy = 0.8858, F1 Class 0 = 0.8555, F1 Class 1 = 0.9056\n\n🔁 Combo 7 → Train:Test = 70:30\n Epoch 1/20, Loss: 4.0144\n Epoch 2/20, Loss: 3.9859\n Epoch 3/20, Loss: 4.0229\n Epoch 4/20, Loss: 4.0330\n Epoch 5/20, Loss: 4.0489\n Epoch 6/20, Loss: 3.9487\n Epoch 7/20, Loss: 3.9719\n Epoch 8/20, Loss: 3.9823\n Epoch 9/20, Loss: 3.9544\n Epoch 10/20, Loss: 3.9355\n Epoch 11/20, Loss: 3.9505\n Epoch 12/20, Loss: 3.9566\n Epoch 13/20, Loss: 3.9504\n Epoch 14/20, Loss: 3.9479\n Epoch 15/20, Loss: 3.9931\n Epoch 16/20, Loss: 4.0043\n Epoch 17/20, Loss: 3.9456\n Epoch 18/20, Loss: 3.9744\n Epoch 19/20, Loss: 3.9714\n Epoch 20/20, Loss: 3.9958\n\u001b[1m5637/5637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step\n\u001b[1m2416/2416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n✅ Finished Combo 7: Accuracy = 0.8863, F1 Class 0 = 0.8557, F1 Class 1 = 0.9061\n\n🔁 Combo 8 → Train:Test = 80:20\n Epoch 1/20, Loss: 4.8193\n Epoch 2/20, Loss: 4.7543\n Epoch 3/20, Loss: 4.7517\n Epoch 4/20, Loss: 4.7468\n Epoch 5/20, Loss: 4.7226\n Epoch 6/20, Loss: 4.7622\n Epoch 7/20, Loss: 4.7071\n Epoch 8/20, Loss: 4.7374\n Epoch 9/20, Loss: 4.7117\n Epoch 10/20, Loss: 4.7209\n Epoch 11/20, Loss: 4.7054\n Epoch 12/20, Loss: 4.7038\n Epoch 13/20, Loss: 4.7133\n Epoch 14/20, Loss: 4.7399\n Epoch 15/20, Loss: 4.6873\n Epoch 16/20, Loss: 4.6876\n Epoch 17/20, Loss: 4.7267\n Epoch 18/20, Loss: 4.6991\n Epoch 19/20, Loss: 4.7061\n Epoch 20/20, Loss: 4.7543\n\u001b[1m6442/6442\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n\u001b[1m1611/1611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n✅ Finished Combo 8: Accuracy = 0.8850, F1 Class 0 = 0.8547, F1 Class 1 = 0.9049\n\n🔁 Combo 9 → Train:Test = 90:10\n Epoch 1/20, Loss: 5.1984\n Epoch 2/20, Loss: 5.1605\n Epoch 3/20, Loss: 5.1561\n Epoch 4/20, Loss: 5.1702\n Epoch 5/20, Loss: 5.1532\n Epoch 6/20, Loss: 5.1396\n Epoch 7/20, Loss: 5.1647\n Epoch 8/20, Loss: 5.1224\n Epoch 9/20, Loss: 5.1491\n Epoch 10/20, Loss: 5.1528\n Epoch 11/20, Loss: 5.1513\n Epoch 12/20, Loss: 5.1312\n Epoch 13/20, Loss: 5.1376\n Epoch 14/20, Loss: 5.1298\n Epoch 15/20, Loss: 5.1448\n Epoch 16/20, Loss: 5.1290\n Epoch 17/20, Loss: 5.1299\n Epoch 18/20, Loss: 5.1283\n Epoch 19/20, Loss: 5.1000\n Epoch 20/20, Loss: 5.0774\n\u001b[1m7248/7248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n\u001b[1m806/806\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n","output_type":"stream"}],"execution_count":null}]}