{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12365068,"sourceType":"datasetVersion","datasetId":7796172}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load and preprocess the data\ntrain = pd.read_parquet(\"/kaggle/input/unsb-nb15/UNSW_NB15_testing-set.parquet\")\ntest = pd.read_parquet(\"/kaggle/input/unsb-nb15/UNSW_NB15_testing-set.parquet\")\ndf = pd.concat([train, test])\ndf = df.drop(columns=['id', 'attack_cat'], errors='ignore')\ncat_cols = ['proto', 'service', 'state']\nfor col in cat_cols:\n    df[col] = pd.factorize(df[col])[0]\nX = df.drop('label', axis=1)\ny = df['label']\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled, columns=X.columns)\n\n# Enhanced MoCo encoder with deeper layers, BatchNorm, Dropout\ndef build_encoderT(input_dim):\n    inputs = layers.Input(shape=(input_dim,))\n    x = layers.Dense(256)(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('swish')(x)\n    x = layers.Dropout(0.3)(x)\n\n    x = layers.Dense(128)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('swish')(x)\n    x = layers.Dropout(0.3)(x)\n\n    x = layers.Dense(64)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('swish')(x)\n\n    projection = layers.Dense(32)(x)\n    prediction = layers.Dense(32)(layers.Activation('swish')(projection))\n    return models.Model(inputs, prediction)\n\n# Contrastive loss\ndef contrastive_loss(query, key, queue, temperature=0.07):\n    query = tf.math.l2_normalize(query, axis=1)\n    key = tf.math.l2_normalize(key, axis=1)\n    queue = tf.math.l2_normalize(queue, axis=1)\n    l_pos = tf.reshape(tf.reduce_sum(query * key, axis=1), [-1,1])\n    l_neg = tf.matmul(query, queue, transpose_b=True)\n    logits = tf.concat([l_pos, l_neg], axis=1) / temperature\n    labels = tf.zeros(logits.shape[0], dtype=tf.int32)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n    return tf.reduce_mean(loss)\n\n# Augmentations\ndef augment_batch(X):\n    mask = (np.random.rand(*X.shape) > 0.15).astype(np.float32)\n    noise = np.random.normal(0, 0.05, size=X.shape).astype(np.float32)\n    return X * mask + noise\n\n# FIFO queue update\ndef update_queue(queue, new_keys):\n    batch_size = tf.shape(new_keys)[0]\n    remaining = queue.shape[0] - batch_size\n    new_queue = tf.concat([new_keys, queue[:remaining]], axis=0)\n    return tf.stop_gradient(new_queue)\n\n# Momentum encoder update\n@tf.function\ndef momentum_update(query_encoder, key_encoder, m=0.999):\n    for q_var, k_var in zip(query_encoder.trainable_variables, key_encoder.trainable_variables):\n        k_var.assign(m * k_var + (1 - m) * q_var)\n\n# Training setup\nembedding_dim = 32\nqueue_size = 65536\nqueue = tf.Variable(tf.math.l2_normalize(tf.random.normal([queue_size, embedding_dim]), axis=1), trainable=False)\n\nratios = [(10,90), (20,80), (30,70), (40,60), (50,50), (60,40), (70,30), (80,20), (90,10)]\nall_results = []\n\nfor train_pct, test_pct in ratios:\n    print(f\"\\n--- Train:Test = {train_pct}:{test_pct} ---\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X.values.astype(np.float32), y.values,\n        test_size=test_pct/100,\n        stratify=y,\n        random_state=42\n    )\n\n    query_encoder = build_encoderT(X_train.shape[1])\n    key_encoder = build_encoderT(X_train.shape[1])\n    for q_var, k_var in zip(query_encoder.variables, key_encoder.variables):\n        k_var.assign(q_var)\n\n    optimizer = tf.keras.optimizers.Adam(3e-4)\n    batch_size = 512\n    epochs = 20\n    num_samples = X_train.shape[0]\n\n    for epoch in range(epochs):\n        idx = np.random.permutation(num_samples)\n        X_shuffled = X_train[idx]\n\n        for i in range(0, num_samples, batch_size):\n            batch = X_shuffled[i:i+batch_size]\n            if batch.shape[0] < 2:\n                continue\n            x_q = augment_batch(batch)\n            x_k = augment_batch(batch)\n            with tf.GradientTape() as tape:\n                z_q = query_encoder(x_q, training=True)\n                z_k = tf.stop_gradient(key_encoder(x_k, training=True))\n                loss = contrastive_loss(z_q, z_k, queue)\n            grads = tape.gradient(loss, query_encoder.trainable_variables)\n            optimizer.apply_gradients(zip(grads, query_encoder.trainable_variables))\n            momentum_update(query_encoder, key_encoder)\n            queue.assign(update_queue(queue, z_k))\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.numpy():.4f}\")\n\n    # Fine-tuning phase\n    query_encoder.trainable = True\n    finetune_model = tf.keras.Sequential([\n        query_encoder,\n        layers.Dense(64, activation='swish'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.3),\n        layers.Dense(1, activation='sigmoid')\n    ])\n\n    finetune_model.compile(\n        optimizer=tf.keras.optimizers.Adam(1e-4),\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n\n    finetune_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=256)\n    loss, acc = finetune_model.evaluate(X_test, y_test)\n    print(f\"Fine-tuned Accuracy: {acc:.4f}\")\n\n    y_pred = (finetune_model.predict(X_test) > 0.5).astype(int)\n    report = classification_report(y_test, y_pred, output_dict=True)\n\n    all_results.append({\n        'train_pct': train_pct,\n        'test_pct': test_pct,\n        'epoch': epoch + 1,\n        'loss': float(loss),\n        'accuracy': acc,\n        'f1_class_0': report['0']['f1-score'],\n        'f1_class_1': report['1']['f1-score']\n    })\n\nresults_df = pd.DataFrame(all_results)\nresults_df.to_csv('moco_finetune_results.csv', index=False)\nprint(\"\\nSaved results to 'moco_finetune_results.csv'\")\n\n# Visualization\nsns.set(style=\"whitegrid\")\nratios_unique = results_df[['train_pct', 'test_pct']].drop_duplicates()\nplt.figure(figsize=(12, 7))\nfor _, row in ratios_unique.iterrows():\n    subset = results_df[(results_df['train_pct'] == row['train_pct']) & \n                        (results_df['test_pct'] == row['test_pct'])]\n    label = f\"{int(row['train_pct'])}:{int(row['test_pct'])}\"\n    plt.plot(subset['epoch'], subset['accuracy'], marker='o', label=label)\nplt.title(\"Accuracy vs Epoch per Train:Test Ratio (Fine-Tuned)\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend(title=\"Train:Test\")\nplt.tight_layout()\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:30:49.521252Z","iopub.execute_input":"2025-07-03T18:30:49.521728Z"}},"outputs":[{"name":"stderr","text":"2025-07-03 18:30:51.286617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751567451.469954      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751567451.523612      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1751567465.476497      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1751567465.477199      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"\n--- Train:Test = 10:90 ---\nEpoch 1/20, Loss: 5.4173\nEpoch 2/20, Loss: 4.7954\nEpoch 3/20, Loss: 5.6169\nEpoch 4/20, Loss: 6.3851\nEpoch 5/20, Loss: 5.7556\nEpoch 6/20, Loss: 5.5960\nEpoch 7/20, Loss: 6.3493\nEpoch 8/20, Loss: 5.6544\nEpoch 9/20, Loss: 5.4031\nEpoch 10/20, Loss: 6.6148\nEpoch 11/20, Loss: 5.2557\nEpoch 12/20, Loss: 5.4638\nEpoch 13/20, Loss: 5.4266\nEpoch 14/20, Loss: 5.8204\nEpoch 15/20, Loss: 6.5851\nEpoch 16/20, Loss: 5.1892\nEpoch 17/20, Loss: 5.0568\nEpoch 18/20, Loss: 4.8865\nEpoch 19/20, Loss: 4.9842\nEpoch 20/20, Loss: 5.1413\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1751567582.741681     101 service.cc:148] XLA service 0x7afa480048d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1751567582.742156     101 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1751567582.742178     101 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1751567583.317167     101 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m56/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5941 - loss: 0.7373","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1751567586.495973     101 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 92ms/step - accuracy: 0.6050 - loss: 0.7255 - val_accuracy: 0.7734 - val_loss: 0.6249\nEpoch 2/10\n\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7480 - loss: 0.5378 - val_accuracy: 0.7770 - val_loss: 0.5636\nEpoch 3/10\n\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7776 - loss: 0.4688 - val_accuracy: 0.7818 - val_loss: 0.4983\nEpoch 4/10\n\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7788 - loss: 0.4382 - val_accuracy: 0.7947 - val_loss: 0.4447\nEpoch 5/10\n\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7761 - loss: 0.4301 - val_accuracy: 0.8074 - val_loss: 0.4025\nEpoch 6/10\n\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7864 - loss: 0.4051 - val_accuracy: 0.8159 - val_loss: 0.3741\nEpoch 7/10\n\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7949 - loss: 0.3980 - val_accuracy: 0.8190 - val_loss: 0.3572\nEpoch 8/10\n\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7994 - loss: 0.3892 - val_accuracy: 0.8215 - val_loss: 0.3443\nEpoch 9/10\n\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7988 - loss: 0.3865 - val_accuracy: 0.8243 - val_loss: 0.3368\nEpoch 10/10\n\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8118 - loss: 0.3608 - val_accuracy: 0.8281 - val_loss: 0.3299\n\u001b[1m4632/4632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - accuracy: 0.8283 - loss: 0.3290\nFine-tuned Accuracy: 0.8281\n\u001b[1m4632/4632\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step\n\n--- Train:Test = 20:80 ---\nEpoch 1/20, Loss: 5.3045\nEpoch 2/20, Loss: 5.8683\nEpoch 3/20, Loss: 5.2938\nEpoch 4/20, Loss: 6.5375\nEpoch 5/20, Loss: 5.3297\nEpoch 6/20, Loss: 5.0162\nEpoch 7/20, Loss: 5.0336\nEpoch 8/20, Loss: 5.3522\nEpoch 9/20, Loss: 5.2554\nEpoch 10/20, Loss: 4.7375\nEpoch 11/20, Loss: 5.2503\nEpoch 12/20, Loss: 4.5396\nEpoch 13/20, Loss: 4.5385\nEpoch 14/20, Loss: 7.6593\nEpoch 15/20, Loss: 4.5355\nEpoch 16/20, Loss: 4.7284\nEpoch 17/20, Loss: 4.3409\nEpoch 18/20, Loss: 4.3047\nEpoch 19/20, Loss: 3.6911\nEpoch 20/20, Loss: 4.1379\nEpoch 1/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 47ms/step - accuracy: 0.6452 - loss: 0.6754 - val_accuracy: 0.7766 - val_loss: 0.5749\nEpoch 2/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7610 - loss: 0.4824 - val_accuracy: 0.7909 - val_loss: 0.4713\nEpoch 3/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7784 - loss: 0.4375 - val_accuracy: 0.8089 - val_loss: 0.3907\nEpoch 4/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7820 - loss: 0.4145 - val_accuracy: 0.8159 - val_loss: 0.3549\nEpoch 5/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7934 - loss: 0.3962 - val_accuracy: 0.8228 - val_loss: 0.3379\nEpoch 6/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7979 - loss: 0.3788 - val_accuracy: 0.8304 - val_loss: 0.3268\nEpoch 7/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8039 - loss: 0.3678 - val_accuracy: 0.8317 - val_loss: 0.3176\nEpoch 8/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8147 - loss: 0.3522 - val_accuracy: 0.8355 - val_loss: 0.3110\nEpoch 9/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8201 - loss: 0.3451 - val_accuracy: 0.8395 - val_loss: 0.3041\nEpoch 10/10\n\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8247 - loss: 0.3354 - val_accuracy: 0.8407 - val_loss: 0.2974\n\u001b[1m4117/4117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.8405 - loss: 0.2955\nFine-tuned Accuracy: 0.8407\n\u001b[1m4117/4117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step\n\n--- Train:Test = 30:70 ---\nEpoch 1/20, Loss: 6.0207\nEpoch 2/20, Loss: 5.4952\nEpoch 3/20, Loss: 5.7268\nEpoch 4/20, Loss: 5.1438\nEpoch 5/20, Loss: 6.2088\nEpoch 6/20, Loss: 5.2540\nEpoch 7/20, Loss: 4.5053\nEpoch 8/20, Loss: 4.2131\nEpoch 9/20, Loss: 4.3301\nEpoch 10/20, Loss: 4.3133\nEpoch 11/20, Loss: 3.7297\nEpoch 12/20, Loss: 3.8872\nEpoch 13/20, Loss: 3.5593\nEpoch 14/20, Loss: 3.9351\nEpoch 15/20, Loss: 3.5354\nEpoch 16/20, Loss: 3.6632\nEpoch 17/20, Loss: 3.5153\nEpoch 18/20, Loss: 3.1462\nEpoch 19/20, Loss: 3.0102\nEpoch 20/20, Loss: 2.8466\nEpoch 1/10\n\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 31ms/step - accuracy: 0.7090 - loss: 0.5926 - val_accuracy: 0.7981 - val_loss: 0.4970\nEpoch 2/10\n\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7726 - loss: 0.4412 - val_accuracy: 0.8132 - val_loss: 0.3858\nEpoch 3/10\n\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.7906 - loss: 0.4007 - val_accuracy: 0.8285 - val_loss: 0.3422\nEpoch 4/10\n\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8006 - loss: 0.3815 - val_accuracy: 0.8330 - val_loss: 0.3261\nEpoch 5/10\n\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8075 - loss: 0.3602 - val_accuracy: 0.8328 - val_loss: 0.3155\nEpoch 6/10\n\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8189 - loss: 0.3417 - val_accuracy: 0.8347 - val_loss: 0.3039\nEpoch 7/10\n\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8243 - loss: 0.3330 - val_accuracy: 0.8442 - val_loss: 0.2935\nEpoch 8/10\n\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8291 - loss: 0.3236 - val_accuracy: 0.8447 - val_loss: 0.2875\nEpoch 9/10\n\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8356 - loss: 0.3135 - val_accuracy: 0.8553 - val_loss: 0.2764\nEpoch 10/10\n\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.8420 - loss: 0.3079 - val_accuracy: 0.8542 - val_loss: 0.2733\n\u001b[1m3603/3603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - accuracy: 0.8538 - loss: 0.2734\nFine-tuned Accuracy: 0.8542\n\u001b[1m3603/3603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n\n--- Train:Test = 40:60 ---\nEpoch 1/20, Loss: 6.0217\nEpoch 2/20, Loss: 4.7972\nEpoch 3/20, Loss: 5.5703\nEpoch 4/20, Loss: 4.8963\nEpoch 5/20, Loss: 4.6662\nEpoch 6/20, Loss: 4.4008\nEpoch 7/20, Loss: 4.1582\nEpoch 8/20, Loss: 3.9467\nEpoch 9/20, Loss: 3.7581\nEpoch 10/20, Loss: 3.6496\nEpoch 11/20, Loss: 3.4975\nEpoch 12/20, Loss: 3.0270\nEpoch 13/20, Loss: 3.1918\nEpoch 14/20, Loss: 3.0743\nEpoch 15/20, Loss: 2.9075\nEpoch 16/20, Loss: 2.8291\nEpoch 17/20, Loss: 2.3112\nEpoch 18/20, Loss: 3.5263\nEpoch 19/20, Loss: 2.7065\nEpoch 20/20, Loss: 2.4195\nEpoch 1/10\n\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 23ms/step - accuracy: 0.6660 - loss: 0.6393 - val_accuracy: 0.7888 - val_loss: 0.4696\nEpoch 2/10\n\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7767 - loss: 0.4377 - val_accuracy: 0.8149 - val_loss: 0.3619\nEpoch 3/10\n\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7964 - loss: 0.3906 - val_accuracy: 0.8285 - val_loss: 0.3372\nEpoch 4/10\n\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8083 - loss: 0.3744 - val_accuracy: 0.8406 - val_loss: 0.3140\nEpoch 5/10\n\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8161 - loss: 0.3493 - val_accuracy: 0.8484 - val_loss: 0.2993\nEpoch 6/10\n\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8271 - loss: 0.3333 - val_accuracy: 0.8517 - val_loss: 0.2897\nEpoch 7/10\n\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8313 - loss: 0.3231 - val_accuracy: 0.8558 - val_loss: 0.2794\nEpoch 8/10\n\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8387 - loss: 0.3115 - val_accuracy: 0.8613 - val_loss: 0.2710\nEpoch 9/10\n\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8456 - loss: 0.2976 - val_accuracy: 0.8602 - val_loss: 0.2671\nEpoch 10/10\n\u001b[1m258/258\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8506 - loss: 0.2913 - val_accuracy: 0.8643 - val_loss: 0.2614\n\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.8627 - loss: 0.2630\nFine-tuned Accuracy: 0.8643\n\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step\n\n--- Train:Test = 50:50 ---\nEpoch 1/20, Loss: 5.9860\nEpoch 2/20, Loss: 5.4065\nEpoch 3/20, Loss: 5.2293\nEpoch 4/20, Loss: 4.8935\nEpoch 5/20, Loss: 4.1720\nEpoch 6/20, Loss: 4.3563\nEpoch 7/20, Loss: 3.8023\nEpoch 8/20, Loss: 3.7389\nEpoch 9/20, Loss: 3.2765\nEpoch 10/20, Loss: 3.4382\nEpoch 11/20, Loss: 3.2996\nEpoch 12/20, Loss: 2.8805\nEpoch 13/20, Loss: 2.7458\nEpoch 14/20, Loss: 2.4061\nEpoch 15/20, Loss: 2.6308\nEpoch 16/20, Loss: 2.8781\nEpoch 17/20, Loss: 2.1511\nEpoch 18/20, Loss: 2.3672\nEpoch 19/20, Loss: 2.2193\nEpoch 20/20, Loss: 2.0802\nEpoch 1/10\n\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.6957 - loss: 0.6047 - val_accuracy: 0.7889 - val_loss: 0.4421\nEpoch 2/10\n\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7785 - loss: 0.4229 - val_accuracy: 0.8125 - val_loss: 0.3497\nEpoch 3/10\n\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7991 - loss: 0.3781 - val_accuracy: 0.8216 - val_loss: 0.3253\nEpoch 4/10\n\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8098 - loss: 0.3522 - val_accuracy: 0.8378 - val_loss: 0.3056\nEpoch 5/10\n\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8224 - loss: 0.3335 - val_accuracy: 0.8415 - val_loss: 0.2912\nEpoch 6/10\n\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8309 - loss: 0.3179 - val_accuracy: 0.8481 - val_loss: 0.2801\nEpoch 7/10\n\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8414 - loss: 0.3023 - val_accuracy: 0.8542 - val_loss: 0.2724\nEpoch 8/10\n\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8445 - loss: 0.2997 - val_accuracy: 0.8573 - val_loss: 0.2665\nEpoch 9/10\n\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8479 - loss: 0.2885 - val_accuracy: 0.8636 - val_loss: 0.2589\nEpoch 10/10\n\u001b[1m322/322\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8523 - loss: 0.2824 - val_accuracy: 0.8653 - val_loss: 0.2547\n\u001b[1m2573/2573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8668 - loss: 0.2539\nFine-tuned Accuracy: 0.8653\n\u001b[1m2573/2573\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n\n--- Train:Test = 60:40 ---\nEpoch 1/20, Loss: 5.7143\nEpoch 2/20, Loss: 4.9144\nEpoch 3/20, Loss: 4.8575\nEpoch 4/20, Loss: 4.2464\nEpoch 5/20, Loss: 3.9543\nEpoch 6/20, Loss: 3.3865\nEpoch 7/20, Loss: 3.6912\nEpoch 8/20, Loss: 3.3475\nEpoch 9/20, Loss: 3.2279\nEpoch 10/20, Loss: 2.6541\nEpoch 11/20, Loss: 2.8543\nEpoch 12/20, Loss: 2.4741\nEpoch 13/20, Loss: 2.2058\nEpoch 14/20, Loss: 2.4888\nEpoch 15/20, Loss: 2.2261\nEpoch 16/20, Loss: 1.9407\nEpoch 17/20, Loss: 2.0600\nEpoch 18/20, Loss: 2.0771\nEpoch 19/20, Loss: 1.8629\nEpoch 20/20, Loss: 1.7674\nEpoch 1/10\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 16ms/step - accuracy: 0.7093 - loss: 0.5804 - val_accuracy: 0.7995 - val_loss: 0.4016\nEpoch 2/10\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7867 - loss: 0.4057 - val_accuracy: 0.8245 - val_loss: 0.3344\nEpoch 3/10\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8036 - loss: 0.3657 - val_accuracy: 0.8341 - val_loss: 0.3116\nEpoch 4/10\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8212 - loss: 0.3356 - val_accuracy: 0.8446 - val_loss: 0.2951\nEpoch 5/10\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8260 - loss: 0.3249 - val_accuracy: 0.8519 - val_loss: 0.2827\nEpoch 6/10\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8337 - loss: 0.3091 - val_accuracy: 0.8513 - val_loss: 0.2757\nEpoch 7/10\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8441 - loss: 0.2977 - val_accuracy: 0.8548 - val_loss: 0.2682\nEpoch 8/10\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8465 - loss: 0.2892 - val_accuracy: 0.8593 - val_loss: 0.2620\nEpoch 9/10\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8494 - loss: 0.2835 - val_accuracy: 0.8665 - val_loss: 0.2555\nEpoch 10/10\n\u001b[1m386/386\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8525 - loss: 0.2818 - val_accuracy: 0.8669 - val_loss: 0.2519\n\u001b[1m2059/2059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8664 - loss: 0.2514\nFine-tuned Accuracy: 0.8669\n\u001b[1m2059/2059\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n\n--- Train:Test = 70:30 ---\nEpoch 1/20, Loss: 5.1721\nEpoch 2/20, Loss: 5.5079\nEpoch 3/20, Loss: 4.6977\nEpoch 4/20, Loss: 4.3574\nEpoch 5/20, Loss: 6.4923\nEpoch 6/20, Loss: 3.0271\nEpoch 7/20, Loss: 3.6509\nEpoch 8/20, Loss: 3.2287\nEpoch 9/20, Loss: 2.9166\nEpoch 10/20, Loss: 3.0521\nEpoch 11/20, Loss: 2.5460\nEpoch 12/20, Loss: 3.3136\n","output_type":"stream"}],"execution_count":null}]}