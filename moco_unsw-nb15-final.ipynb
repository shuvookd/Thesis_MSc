{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11589390,"sourceType":"datasetVersion","datasetId":7267066}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:55:41.545548Z","iopub.execute_input":"2025-07-03T17:55:41.546233Z","iopub.status.idle":"2025-07-03T17:55:41.550596Z","shell.execute_reply.started":"2025-07-03T17:55:41.546207Z","shell.execute_reply":"2025-07-03T17:55:41.549781Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load and preprocess the data\ntrain = pd.read_parquet(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.parquet\")\ntest = pd.read_parquet(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.parquet\")\ndf = pd.concat([train, test])\ndf = df.drop(columns=['id', 'attack_cat'], errors='ignore')\ncat_cols = ['proto', 'service', 'state']\nfor col in cat_cols:\n    df[col] = pd.factorize(df[col])[0]\nX = df.drop('label', axis=1)\ny = df['label']\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled, columns=X.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:55:49.683576Z","iopub.execute_input":"2025-07-03T17:55:49.684086Z","iopub.status.idle":"2025-07-03T17:55:49.986744Z","shell.execute_reply.started":"2025-07-03T17:55:49.684064Z","shell.execute_reply":"2025-07-03T17:55:49.985929Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# MoCo encoder with prediction head (like MoCo v2)\ndef build_encoder(input_dim):\n    inputs = layers.Input(shape=(input_dim,))\n    x = layers.Dense(256, activation='swish')(inputs)\n    x = layers.Dense(128, activation='swish')(x)\n    projection = layers.Dense(64)(x)  # Projection head\n    prediction = layers.Dense(64)(layers.Activation('swish')(projection))  # Prediction head\n    return models.Model(inputs, prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:55:52.195139Z","iopub.execute_input":"2025-07-03T17:55:52.195877Z","iopub.status.idle":"2025-07-03T17:55:52.200280Z","shell.execute_reply.started":"2025-07-03T17:55:52.195851Z","shell.execute_reply":"2025-07-03T17:55:52.199590Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Contrastive loss\ndef contrastive_loss(query, key, queue, temperature=0.07):\n    query = tf.math.l2_normalize(query, axis=1)\n    key = tf.math.l2_normalize(key, axis=1)\n    queue = tf.math.l2_normalize(queue, axis=1)\n    l_pos = tf.reshape(tf.reduce_sum(query * key, axis=1), [-1,1])\n    l_neg = tf.matmul(query, queue, transpose_b=True)\n    logits = tf.concat([l_pos, l_neg], axis=1) / temperature\n    labels = tf.zeros(logits.shape[0], dtype=tf.int32)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n    return tf.reduce_mean(loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:55:54.407060Z","iopub.execute_input":"2025-07-03T17:55:54.407746Z","iopub.status.idle":"2025-07-03T17:55:54.412633Z","shell.execute_reply.started":"2025-07-03T17:55:54.407719Z","shell.execute_reply":"2025-07-03T17:55:54.411869Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Augmentations\ndef augment_batch(X):\n    mask = (np.random.rand(*X.shape) > 0.15).astype(np.float32)\n    noise = np.random.normal(0, 0.05, size=X.shape).astype(np.float32)\n    return X * mask + noise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:55:56.584604Z","iopub.execute_input":"2025-07-03T17:55:56.585221Z","iopub.status.idle":"2025-07-03T17:55:56.589238Z","shell.execute_reply.started":"2025-07-03T17:55:56.585196Z","shell.execute_reply":"2025-07-03T17:55:56.588424Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# FIFO queue update\ndef update_queue(queue, new_keys):\n    batch_size = tf.shape(new_keys)[0]\n    remaining = queue.shape[0] - batch_size\n    new_queue = tf.concat([new_keys, queue[:remaining]], axis=0)\n    return tf.stop_gradient(new_queue)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:55:58.785576Z","iopub.execute_input":"2025-07-03T17:55:58.786305Z","iopub.status.idle":"2025-07-03T17:55:58.791549Z","shell.execute_reply.started":"2025-07-03T17:55:58.786270Z","shell.execute_reply":"2025-07-03T17:55:58.790580Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Momentum encoder update\n@tf.function\ndef momentum_update(query_encoder, key_encoder, m=0.999):\n    for q_var, k_var in zip(query_encoder.trainable_variables, key_encoder.trainable_variables):\n        k_var.assign(m * k_var + (1 - m) * q_var)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:56:00.817418Z","iopub.execute_input":"2025-07-03T17:56:00.818008Z","iopub.status.idle":"2025-07-03T17:56:00.824432Z","shell.execute_reply.started":"2025-07-03T17:56:00.817986Z","shell.execute_reply":"2025-07-03T17:56:00.823604Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Training setup\nembedding_dim = 64\nqueue_size = 65536\nqueue = tf.Variable(tf.math.l2_normalize(tf.random.normal([queue_size, embedding_dim]), axis=1), trainable=False)\n\nratios = [(10,90), (20,80), (30,70), (40,60), (50,50), (60,40), (70,30), (80,20), (90,10)]\nall_results = []\n\nfor train_pct, test_pct in ratios:\n    print(f\"\\n--- Train:Test = {train_pct}:{test_pct} ---\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X.values.astype(np.float32), y.values,\n        test_size=test_pct/100,\n        stratify=y,\n        random_state=42\n    )\n\n    query_encoder = build_encoder(X_train.shape[1])\n    key_encoder = build_encoder(X_train.shape[1])\n    for q_var, k_var in zip(query_encoder.variables, key_encoder.variables):\n        k_var.assign(q_var)\n\n    optimizer = tf.keras.optimizers.Adam(3e-4)\n    batch_size = 512\n    epochs = 20\n    num_samples = X_train.shape[0]\n\n    for epoch in range(epochs):\n        idx = np.random.permutation(num_samples)\n        X_shuffled = X_train[idx]\n\n        for i in range(0, num_samples, batch_size):\n            batch = X_shuffled[i:i+batch_size]\n            if batch.shape[0] < 2:\n                continue\n            x_q = augment_batch(batch)\n            x_k = augment_batch(batch)\n            with tf.GradientTape() as tape:\n                z_q = query_encoder(x_q, training=True)\n                z_k = tf.stop_gradient(key_encoder(x_k, training=True))\n                loss = contrastive_loss(z_q, z_k, queue)\n            grads = tape.gradient(loss, query_encoder.trainable_variables)\n            optimizer.apply_gradients(zip(grads, query_encoder.trainable_variables))\n            momentum_update(query_encoder, key_encoder)\n            queue.assign(update_queue(queue, z_k))\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.numpy():.4f}\")\n        X_train_ssl = query_encoder.predict(X_train)\n        X_test_ssl = query_encoder.predict(X_test)\n\n        rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n        rf.fit(X_train_ssl, y_train)\n        y_pred = rf.predict(X_test_ssl)\n        report = classification_report(y_test, y_pred, output_dict=True)\n\n        acc = report['accuracy']\n        f1_0 = report['0']['f1-score']\n        f1_1 = report['1']['f1-score']\n\n        all_results.append({\n            'train_pct': train_pct,\n            'test_pct': test_pct,\n            'epoch': epoch + 1,\n            'loss': float(loss.numpy()),\n            'accuracy': acc,\n            'f1_class_0': f1_0,\n            'f1_class_1': f1_1\n        })\n\n        print(f\"Accuracy: {acc:.4f}, F1 Class 0: {f1_0:.4f}, F1 Class 1: {f1_1:.4f}\")\n\nresults_df = pd.DataFrame(all_results)\nresults_df.to_csv('moco_results_all_epochs.csv', index=False)\nprint(\"\\nSaved results to 'moco_results_all_epochs.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization\nsns.set(style=\"whitegrid\")\nratios_unique = results_df[['train_pct', 'test_pct']].drop_duplicates()\nplt.figure(figsize=(12, 7))\nfor _, row in ratios_unique.iterrows():\n    subset = results_df[(results_df['train_pct'] == row['train_pct']) & \n                        (results_df['test_pct'] == row['test_pct'])]\n    label = f\"{int(row['train_pct'])}:{int(row['test_pct'])}\"\n    plt.plot(subset['epoch'], subset['accuracy'], marker='o', label=label)\nplt.title(\"Accuracy vs Epoch per Train:Test Ratio\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend(title=\"Train:Test\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loss vs Epoch\nplt.figure(figsize=(12, 7))\nfor _, row in ratios_unique.iterrows():\n    subset = results_df[\n        (results_df['train_pct'] == row['train_pct']) & \n        (results_df['test_pct'] == row['test_pct'])\n    ]\n    label = f\"{int(row['train_pct'])}:{int(row['test_pct'])}\"\n    plt.plot(subset['epoch'], subset['loss'], marker='o', label=label)\nplt.title(\"Contrastive Loss vs Epoch per Train:Test Ratio\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend(title=\"Train:Test\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}