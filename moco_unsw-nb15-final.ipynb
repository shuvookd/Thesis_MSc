{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11589390,"sourceType":"datasetVersion","datasetId":7267066}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T05:48:38.964472Z","iopub.execute_input":"2025-07-04T05:48:38.965044Z","iopub.status.idle":"2025-07-04T05:48:54.466051Z","shell.execute_reply.started":"2025-07-04T05:48:38.965017Z","shell.execute_reply":"2025-07-04T05:48:54.465441Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"2025-07-04 05:48:40.980389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751608121.236863      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751608121.311444      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Load and preprocess the data\ntrain = pd.read_parquet(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.parquet\")\ntest = pd.read_parquet(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.parquet\")\ndf = pd.concat([train, test])\ndf = df.drop(columns=['id', 'attack_cat'], errors='ignore')\ncat_cols = ['proto', 'service', 'state']\nfor col in cat_cols:\n    df[col] = pd.factorize(df[col])[0]\nX = df.drop('label', axis=1)\ny = df['label']\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled, columns=X.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T05:49:03.091270Z","iopub.execute_input":"2025-07-04T05:49:03.092063Z","iopub.status.idle":"2025-07-04T05:49:03.795189Z","shell.execute_reply.started":"2025-07-04T05:49:03.092037Z","shell.execute_reply":"2025-07-04T05:49:03.794622Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# MoCo encoder with prediction head (like MoCo v2)\ndef build_encoder(input_dim):\n    inputs = layers.Input(shape=(input_dim,))\n    x = layers.Dense(256, activation='swish')(inputs)\n    x = layers.Dense(128, activation='swish')(x)\n    projection = layers.Dense(64)(x)  # Projection head\n    prediction = layers.Dense(64)(layers.Activation('swish')(projection))  # Prediction head\n    return models.Model(inputs, prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T05:49:06.739299Z","iopub.execute_input":"2025-07-04T05:49:06.739887Z","iopub.status.idle":"2025-07-04T05:49:06.745220Z","shell.execute_reply.started":"2025-07-04T05:49:06.739859Z","shell.execute_reply":"2025-07-04T05:49:06.744267Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Contrastive loss\ndef contrastive_loss(query, key, queue, temperature=0.07):\n    query = tf.math.l2_normalize(query, axis=1)\n    key = tf.math.l2_normalize(key, axis=1)\n    queue = tf.math.l2_normalize(queue, axis=1)\n    l_pos = tf.reshape(tf.reduce_sum(query * key, axis=1), [-1,1])\n    l_neg = tf.matmul(query, queue, transpose_b=True)\n    logits = tf.concat([l_pos, l_neg], axis=1) / temperature\n    labels = tf.zeros(logits.shape[0], dtype=tf.int32)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n    return tf.reduce_mean(loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T05:49:10.229850Z","iopub.execute_input":"2025-07-04T05:49:10.230378Z","iopub.status.idle":"2025-07-04T05:49:10.235405Z","shell.execute_reply.started":"2025-07-04T05:49:10.230351Z","shell.execute_reply":"2025-07-04T05:49:10.234754Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Augmentations\ndef augment_batch(X):\n    mask = (np.random.rand(*X.shape) > 0.15).astype(np.float32)\n    noise = np.random.normal(0, 0.05, size=X.shape).astype(np.float32)\n    return X * mask + noise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T05:49:13.497332Z","iopub.execute_input":"2025-07-04T05:49:13.497609Z","iopub.status.idle":"2025-07-04T05:49:13.501909Z","shell.execute_reply.started":"2025-07-04T05:49:13.497586Z","shell.execute_reply":"2025-07-04T05:49:13.501168Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# FIFO queue update\ndef update_queue(queue, new_keys):\n    batch_size = tf.shape(new_keys)[0]\n    remaining = queue.shape[0] - batch_size\n    new_queue = tf.concat([new_keys, queue[:remaining]], axis=0)\n    return tf.stop_gradient(new_queue)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T05:49:16.875727Z","iopub.execute_input":"2025-07-04T05:49:16.875996Z","iopub.status.idle":"2025-07-04T05:49:16.880398Z","shell.execute_reply.started":"2025-07-04T05:49:16.875974Z","shell.execute_reply":"2025-07-04T05:49:16.879640Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Momentum encoder update\n@tf.function\ndef momentum_update(query_encoder, key_encoder, m=0.999):\n    for q_var, k_var in zip(query_encoder.trainable_variables, key_encoder.trainable_variables):\n        k_var.assign(m * k_var + (1 - m) * q_var)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T05:49:20.235282Z","iopub.execute_input":"2025-07-04T05:49:20.235975Z","iopub.status.idle":"2025-07-04T05:49:20.240526Z","shell.execute_reply.started":"2025-07-04T05:49:20.235951Z","shell.execute_reply":"2025-07-04T05:49:20.239768Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Training setup\nembedding_dim = 64\nqueue_size = 65536\nqueue = tf.Variable(tf.math.l2_normalize(tf.random.normal([queue_size, embedding_dim]), axis=1), trainable=False)\n\nratios = [(10,90), (20,80), (30,70), (40,60), (50,50), (60,40), (70,30), (80,20), (90,10)]\nall_results = []\n\nfor train_pct, test_pct in ratios:\n    print(f\"\\n--- Train:Test = {train_pct}:{test_pct} ---\")\n    X_train, X_test, y_train, y_test = train_test_split(\n        X.values.astype(np.float32), y.values,\n        test_size=test_pct/100,\n        stratify=y,\n        random_state=42\n    )\n\n    query_encoder = build_encoder(X_train.shape[1])\n    key_encoder = build_encoder(X_train.shape[1])\n    for q_var, k_var in zip(query_encoder.variables, key_encoder.variables):\n        k_var.assign(q_var)\n\n    optimizer = tf.keras.optimizers.Adam(3e-4)\n    batch_size = 512\n    epochs = 20\n    num_samples = X_train.shape[0]\n\n    for epoch in range(epochs):\n        idx = np.random.permutation(num_samples)\n        X_shuffled = X_train[idx]\n\n        for i in range(0, num_samples, batch_size):\n            batch = X_shuffled[i:i+batch_size]\n            if batch.shape[0] < 2:\n                continue\n            x_q = augment_batch(batch)\n            x_k = augment_batch(batch)\n            with tf.GradientTape() as tape:\n                z_q = query_encoder(x_q, training=True)\n                z_k = tf.stop_gradient(key_encoder(x_k, training=True))\n                loss = contrastive_loss(z_q, z_k, queue)\n            grads = tape.gradient(loss, query_encoder.trainable_variables)\n            optimizer.apply_gradients(zip(grads, query_encoder.trainable_variables))\n            momentum_update(query_encoder, key_encoder)\n            queue.assign(update_queue(queue, z_k))\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.numpy():.4f}\")\n        X_train_ssl = query_encoder.predict(X_train)\n        X_test_ssl = query_encoder.predict(X_test)\n\n        rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n        rf.fit(X_train_ssl, y_train)\n        y_pred = rf.predict(X_test_ssl)\n        report = classification_report(y_test, y_pred, output_dict=True)\n\n        acc = report['accuracy']\n        f1_0 = report['0']['f1-score']\n        f1_1 = report['1']['f1-score']\n\n        all_results.append({\n            'train_pct': train_pct,\n            'test_pct': test_pct,\n            'epoch': epoch + 1,\n            'loss': float(loss.numpy()),\n            'accuracy': acc,\n            'f1_class_0': f1_0,\n            'f1_class_1': f1_1\n        })\n\n        print(f\"Accuracy: {acc:.4f}, F1 Class 0: {f1_0:.4f}, F1 Class 1: {f1_1:.4f}\")\n\nresults_df = pd.DataFrame(all_results)\nresults_df.to_csv('moco_results_all_epochs.csv', index=False)\nprint(\"\\nSaved results to 'moco_results_all_epochs.csv'\")","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualization\nsns.set(style=\"whitegrid\")\nratios_unique = results_df[['train_pct', 'test_pct']].drop_duplicates()\nplt.figure(figsize=(12, 7))\nfor _, row in ratios_unique.iterrows():\n    subset = results_df[(results_df['train_pct'] == row['train_pct']) & \n                        (results_df['test_pct'] == row['test_pct'])]\n    label = f\"{int(row['train_pct'])}:{int(row['test_pct'])}\"\n    plt.plot(subset['epoch'], subset['accuracy'], marker='o', label=label)\nplt.title(\"Accuracy vs Epoch per Train:Test Ratio\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend(title=\"Train:Test\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loss vs Epoch\nplt.figure(figsize=(12, 7))\nfor _, row in ratios_unique.iterrows():\n    subset = results_df[\n        (results_df['train_pct'] == row['train_pct']) & \n        (results_df['test_pct'] == row['test_pct'])\n    ]\n    label = f\"{int(row['train_pct'])}:{int(row['test_pct'])}\"\n    plt.plot(subset['epoch'], subset['loss'], marker='o', label=label)\nplt.title(\"Contrastive Loss vs Epoch per Train:Test Ratio\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend(title=\"Train:Test\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Hyperparameters Settings for only 70:30 ratio","metadata":{}},{"cell_type":"code","source":"from itertools import product\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T06:08:22.247532Z","iopub.execute_input":"2025-07-04T06:08:22.248050Z","iopub.status.idle":"2025-07-04T06:08:22.252082Z","shell.execute_reply.started":"2025-07-04T06:08:22.248022Z","shell.execute_reply":"2025-07-04T06:08:22.251398Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"learning_rates = [1e-4, 5e-4]\ntemperatures = [0.05, 0.1]\nmomentums = [0.99, 0.995]\ndropouts = [0.3, 0.4]\nbatch_sizes = [128, 256]\nqueue_sizes = [16384, 32768]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T06:08:24.557360Z","iopub.execute_input":"2025-07-04T06:08:24.557962Z","iopub.status.idle":"2025-07-04T06:08:24.561991Z","shell.execute_reply.started":"2025-07-04T06:08:24.557939Z","shell.execute_reply":"2025-07-04T06:08:24.561114Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def augment_batch(X):\n    mask = (np.random.rand(*X.shape) > 0.1).astype(np.float32)\n    noise = np.random.normal(0, 0.03, size=X.shape).astype(np.float32)\n    return X * mask + noise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T06:08:26.055020Z","iopub.execute_input":"2025-07-04T06:08:26.055565Z","iopub.status.idle":"2025-07-04T06:08:26.059434Z","shell.execute_reply.started":"2025-07-04T06:08:26.055543Z","shell.execute_reply":"2025-07-04T06:08:26.058702Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def build_encoder(input_dim, dropout):\n    inputs = layers.Input(shape=(input_dim,))\n    x = layers.Dense(512)(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('swish')(x)\n    x = layers.Dropout(dropout)(x)\n\n    x = layers.Dense(256)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('swish')(x)\n    x = layers.Dropout(dropout)(x)\n\n    x = layers.Dense(128)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('swish')(x)\n\n    projection = layers.Dense(64)(x)\n    prediction = layers.Dense(64)(layers.Activation('swish')(projection))\n    return models.Model(inputs, prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T06:08:28.091993Z","iopub.execute_input":"2025-07-04T06:08:28.092745Z","iopub.status.idle":"2025-07-04T06:08:28.098179Z","shell.execute_reply.started":"2025-07-04T06:08:28.092718Z","shell.execute_reply":"2025-07-04T06:08:28.097424Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import time\nfrom itertools import product\nfrom sklearn.metrics import classification_report\n\nall_results = []\ncombo_id = 0\n\n# Define your hyperparameter grid\nlearning_rates = [1e-4, 5e-4]\ntemperatures = [0.05, 0.1]\nmomentums = [0.99, 0.995]\ndropouts = [0.3, 0.4]\nbatch_sizes = [128, 256]\nqueue_sizes = [16384, 32768]\n\n# Start tuning\nfor lr, temp, m, drop, batch_size, queue_size in product(\n    learning_rates, temperatures, momentums, dropouts, batch_sizes, queue_sizes\n):\n    combo_id += 1\n    print(f\"\\n🔧 Combo {combo_id}: LR={lr}, Temp={temp}, Momentum={m}, Dropout={drop}, \"\n          f\"Batch={batch_size}, Queue={queue_size}\")\n    \n    combo_start = time.time()  # Track time for each combo\n\n    # Build queue\n    queue = tf.Variable(\n        tf.math.l2_normalize(tf.random.normal([queue_size, 64]), axis=1), trainable=False\n    )\n\n    # Split data (70:30)\n    X_train, X_test, y_train, y_test = train_test_split(\n        X.values.astype(np.float32), y.values,\n        test_size=0.3,\n        stratify=y,\n        random_state=42\n    )\n\n    # Build encoders\n    query_encoder = build_encoder(X_train.shape[1], drop)\n    key_encoder = build_encoder(X_train.shape[1], drop)\n    for q_var, k_var in zip(query_encoder.variables, key_encoder.variables):\n        k_var.assign(q_var)\n\n    optimizer = tf.keras.optimizers.Adam(lr)\n    epochs = 20  \n    num_samples = X_train.shape[0]\n\n    # MoCo Pretraining\n    for epoch in range(epochs):\n        idx = np.random.permutation(num_samples)\n        X_shuffled = X_train[idx]\n        total_loss = []\n\n        for i in range(0, num_samples, batch_size):\n            batch = X_shuffled[i:i+batch_size]\n            if batch.shape[0] < 2:\n                continue\n\n            x_q = augment_batch(batch)\n            x_k = augment_batch(batch)\n\n            with tf.GradientTape() as tape:\n                z_q = query_encoder(x_q, training=True)\n                z_k = tf.stop_gradient(key_encoder(x_k, training=True))\n\n                def contrastive_loss(query, key, queue, temperature):\n                    query = tf.math.l2_normalize(query, axis=1)\n                    key = tf.math.l2_normalize(key, axis=1)\n                    queue = tf.math.l2_normalize(queue, axis=1)\n                    l_pos = tf.reshape(tf.reduce_sum(query * key, axis=1), [-1,1])\n                    l_neg = tf.matmul(query, queue, transpose_b=True)\n                    logits = tf.concat([l_pos, l_neg], axis=1) / temperature\n                    labels = tf.zeros(logits.shape[0], dtype=tf.int32)\n                    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)\n                    return tf.reduce_mean(loss)\n\n                loss = contrastive_loss(z_q, z_k, queue, temperature=temp)\n\n            grads = tape.gradient(loss, query_encoder.trainable_variables)\n            optimizer.apply_gradients(zip(grads, query_encoder.trainable_variables))\n\n            for q_var, k_var in zip(query_encoder.trainable_variables, key_encoder.trainable_variables):\n                k_var.assign(m * k_var + (1 - m) * q_var)\n\n            def update_queue(queue, new_keys):\n                batch_size = tf.shape(new_keys)[0]\n                remaining = queue.shape[0] - batch_size\n                return tf.concat([new_keys, queue[:remaining]], axis=0)\n\n            queue.assign(update_queue(queue, z_k))\n            total_loss.append(loss.numpy())\n\n        avg_loss = np.mean(total_loss)\n        print(f\" Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n\n    # Fine-tuning\n    query_encoder.trainable = True\n    finetune_model = tf.keras.Sequential([\n        query_encoder,\n        layers.Dense(128, activation='swish'),\n        layers.BatchNormalization(),\n        layers.Dropout(drop),\n        layers.Dense(1, activation='sigmoid')\n    ])\n\n    finetune_model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr),\n        loss='binary_crossentropy',\n        metrics=['accuracy']\n    )\n\n    finetune_model.fit(X_train, y_train, validation_data=(X_test, y_test),\n                       epochs=5, batch_size=batch_size, verbose=0)\n    loss, acc = finetune_model.evaluate(X_test, y_test, verbose=0)\n    y_pred = (finetune_model.predict(X_test) > 0.5).astype(int)\n    report = classification_report(y_test, y_pred, output_dict=True)\n\n    # Store results\n    combo_result = {\n        'combo_id': combo_id,\n        'learning_rate': lr,\n        'temperature': temp,\n        'momentum': m,\n        'dropout': drop,\n        'batch_size': batch_size,\n        'queue_size': queue_size,\n        'loss': float(loss),\n        'accuracy': acc,\n        'f1_class_0': report['0']['f1-score'],\n        'f1_class_1': report['1']['f1-score']\n    }\n    all_results.append(combo_result)\n\n    # ✅ Print live progress\n    print(f\"✅ Finished Combo {combo_id}: \"\n          f\"Accuracy={acc:.4f}, \"\n          f\"Loss={loss:.4f}, \"\n          f\"F1 Class 0={combo_result['f1_class_0']:.4f}, \"\n          f\"F1 Class 1={combo_result['f1_class_1']:.4f}\")\n\n    # ⏱️ Print time taken for combo\n    elapsed = time.time() - combo_start\n    print(f\"⏱️ Combo Time: {elapsed:.2f} seconds\")\n\n# Save final results\nresults_df = pd.DataFrame(all_results)\nresults_df.to_csv('moco_hyperparam_live_results.csv', index=False)\nprint(\"📁 Saved all results to 'moco_hyperparam_live_results.csv'\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T06:32:31.922427Z","iopub.execute_input":"2025-07-04T06:32:31.922726Z"}},"outputs":[{"name":"stdout","text":"\n🔧 Combo 1: LR=0.0001, Temp=0.05, Momentum=0.99, Dropout=0.3, Batch=128, Queue=16384\n Epoch 1/20, Loss: 0.6462\n Epoch 2/20, Loss: 0.1385\n Epoch 3/20, Loss: 0.0588\n Epoch 4/20, Loss: 0.0351\n Epoch 5/20, Loss: 0.0231\n Epoch 6/20, Loss: 0.0165\n Epoch 7/20, Loss: 0.0126\n Epoch 8/20, Loss: 0.0107\n Epoch 9/20, Loss: 0.0076\n Epoch 10/20, Loss: 0.0073\n Epoch 11/20, Loss: 0.0057\n Epoch 12/20, Loss: 0.0048\n Epoch 13/20, Loss: 0.0049\n Epoch 14/20, Loss: 0.0040\n Epoch 15/20, Loss: 0.0040\n Epoch 16/20, Loss: 0.0040\n Epoch 17/20, Loss: 0.0033\n Epoch 18/20, Loss: 0.0032\n Epoch 19/20, Loss: 0.0033\n Epoch 20/20, Loss: 0.0030\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1751615123.355552     109 service.cc:148] XLA service 0x7f0624009bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1751615123.357479     109 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1751615123.357500     109 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1751615123.995489     109 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1751615126.996876     109 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2416/2416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n✅ Finished Combo 1: Accuracy=0.8939, Loss=0.1987, F1 Class 0=0.8324, F1 Class 1=0.9223\n⏱️ Combo Time: 4405.03 seconds\n\n🔧 Combo 2: LR=0.0001, Temp=0.05, Momentum=0.99, Dropout=0.3, Batch=128, Queue=32768\n Epoch 1/20, Loss: 0.7577\n Epoch 2/20, Loss: 0.1782\n Epoch 3/20, Loss: 0.0776\n Epoch 4/20, Loss: 0.0430\n Epoch 5/20, Loss: 0.0286\n Epoch 6/20, Loss: 0.0210\n Epoch 7/20, Loss: 0.0163\n Epoch 8/20, Loss: 0.0128\n Epoch 9/20, Loss: 0.0102\n Epoch 10/20, Loss: 0.0091\n Epoch 11/20, Loss: 0.0076\n Epoch 12/20, Loss: 0.0072\n Epoch 13/20, Loss: 0.0066\n Epoch 14/20, Loss: 0.0062\n Epoch 15/20, Loss: 0.0056\n Epoch 16/20, Loss: 0.0051\n Epoch 17/20, Loss: 0.0049\n Epoch 18/20, Loss: 0.0049\n Epoch 19/20, Loss: 0.0044\n Epoch 20/20, Loss: 0.0044\n\u001b[1m2416/2416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n✅ Finished Combo 2: Accuracy=0.8931, Loss=0.1988, F1 Class 0=0.8305, F1 Class 1=0.9219\n⏱️ Combo Time: 4498.30 seconds\n\n🔧 Combo 3: LR=0.0001, Temp=0.05, Momentum=0.99, Dropout=0.3, Batch=256, Queue=16384\n Epoch 1/20, Loss: 0.8705\n Epoch 2/20, Loss: 0.2364\n Epoch 3/20, Loss: 0.1097\n Epoch 4/20, Loss: 0.0655\n Epoch 5/20, Loss: 0.0463\n Epoch 6/20, Loss: 0.0323\n Epoch 7/20, Loss: 0.0259\n Epoch 8/20, Loss: 0.0202\n Epoch 9/20, Loss: 0.0173\n Epoch 10/20, Loss: 0.0144\n Epoch 11/20, Loss: 0.0112\n Epoch 12/20, Loss: 0.0103\n Epoch 13/20, Loss: 0.0085\n Epoch 14/20, Loss: 0.0070\n Epoch 15/20, Loss: 0.0060\n Epoch 16/20, Loss: 0.0056\n Epoch 17/20, Loss: 0.0055\n Epoch 18/20, Loss: 0.0046\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"results_df = pd.DataFrame(all_results)\nresults_df.to_csv('moco_hyperparam_extended_results.csv', index=False)\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(14, 6))\nsns.barplot(data=results_df, x='combo_id', y='accuracy', hue='temperature')\nplt.title(\"Accuracy by Hyperparameter Combination\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(14, 6))\nsns.barplot(data=results_df, x='combo_id', y='f1_class_1', hue='learning_rate')\nplt.title(\"F1 Score (Class 1) by Combination\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}